@ARTICLE{cpak-paper,
  title     = "Coronal Plane Alignment of the Knee ({CPAK}) classification",
  author    = "MacDessi, Samuel J and Griffiths-Jones, William and Harris, Ian
               A and Bellemans, Johan and Chen, Darren B",
  abstract  = "AimsA comprehensive classification for coronal lower limb
               alignment with predictive capabilities for knee balance would be
               beneficial in total knee arthroplasty (TKA). This paper
               describes the Coronal Plane Alignment of the Knee (CPAK)
               classification and examines its utility in preoperative soft
               tissue balance prediction, comparing kinematic alignment (KA) to
               mechanical alignment (MA).MethodsA radiological analysis of 500
               healthy and 500 osteoarthritic (OA) knees was used to assess the
               applicability of the CPAK classification. CPAK comprises nine
               phenotypes based on the arithmetic HKA (aHKA) that estimates
               constitutional limb alignment and joint line obliquity (JLO).
               Intraoperative balance was compared within each phenotype in a
               cohort of 138 computer-assisted TKAs randomized to KA or MA.
               Primary outcomes included descriptive analyses of healthy and OA
               groups per CPAK type, and comparison of balance at 10° of
               flexion within each type. Secondary outcomes assessed balance at
               45° and 90° and bone recuts required to achieve final knee
               balance within each CPAK type.ResultsThere was similar frequency
               distribution between healthy and arthritic groups across all
               CPAK types. The most common categories were Type II (39.2\%
               healthy vs 32.2\% OA), Type I (26.4\% healthy vs 19.4\% OA) and
               Type V (15.4\% healthy vs 14.6\% OA). CPAK Types VII, VIII, and
               IX were rare in both populations. Across all CPAK types, a
               greater proportion of KA TKAs achieved optimal balance compared
               to MA. This effect was largest, and statistically significant,
               in CPAK Types I (100\% KA vs 15\% MA; p ConclusionCPAK is a
               pragmatic, comprehensive classification for coronal knee
               alignment, based on constitutional alignment and JLO, that can
               be used in healthy and arthritic knees. CPAK identifies which
               knee phenotypes may benefit most from KA when optimization of
               soft tissue balance is prioritized. Further, it will allow for
               consistency of reporting in future studies. Cite this article:
               Bone Joint J 2021;103-B(2):329--337.",
  journal   = "Bone Joint J.",
  publisher = "British Editorial Society of Bone \& Joint Surgery",
  volume    = "103-B",
  number    =  2,
  pages     = "329--337",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@article{svm-paper,
	abstract = {Single-output regression is a widely used statistical modeling method to predict an output based on one or more features of a datapoint. If a dataset has multiple outputs, they can be predicted independently from each other although this disregards potential correlations and thus may negatively affect the predictive performance. Therefore, multi-output regression methods predict multiple outputs simultaneously. One way to approach single-output regression is by using methods based on support vectors such as support vector regression (SVR) or least-squares SVR (LS-SVR). Based on these two, previous works have devised multi-output support vector regression methods. In this review, we introduce a unified notation to summarize the single-output support vector regression methods SVR and LS-SVR as well as state-of-the-art multi-output support vector regression methods. Furthermore, we implemented a workflow for subject- and record-wise bootstrapping and nested cross-validation experiments, which we used for an exhaustive evaluation of all single- and multi-output support vector regression methods on synthetic and non-synthetic datasets. Although the reviewed papers claim that their multi-output methods improve regression performance, we find that none of them outperform both single-output methods SVR and LS-SVR for various reasons. Due to these results, we reflected about the general concept of support vector regression and then concluded that support vector regression methods do not appear to be suitable for the task of multi-output regression.},
	author = {Nguyen Khoa Tran and Laura C. K{\"u}hle and Gunnar W. Klau},
	doi = {https://doi.org/10.1016/j.patrec.2023.12.007},
	issn = {0167-8655},
	journal = {Pattern Recognition Letters},
	keywords = {Multi-output regression, Support vector, Least-squares},
	pages = {69-75},
	title = {A critical review of multi-output support vector regression},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865523003549},
	volume = {178},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167865523003549},
	bdsk-url-2 = {https://doi.org/10.1016/j.patrec.2023.12.007}
}

@article{goodness-of-fit,
	abstract = {Summary
Success in the use of computer models for simulating environmental variables and processes requires objective model calibration and verification procedures. Several methods for quantifying the goodness-of-fit of observations against model-calculated values have been proposed but none of them is free of limitations and are often ambiguous. When a single indicator is used it may lead to incorrect verification of the model. Instead, a combination of graphical results, absolute value error statistics (i.e. root mean square error), and normalized goodness-of-fit statistics (i.e. Nash--Sutcliffe Efficiency coefficient, NSE) is currently recommended. Interpretation of NSE values is often subjective, and may be biased by the magnitude and number of data points, data outliers and repeated data. The statistical significance of the performance statistics is an aspect generally ignored that helps in reducing subjectivity in the proper interpretation of the model performance. In this work, approximated probability distributions for two common indicators (NSE and root mean square error) are derived with bootstrapping (block bootstrapping when dealing with time series), followed by bias corrected and accelerated calculation of confidence intervals. Hypothesis testing of the indicators exceeding threshold values is proposed in a unified framework for statistically accepting or rejecting the model performance. It is illustrated how model performance is not linearly related with NSE, which is critical for its proper interpretation. Additionally, the sensitivity of the indicators to model bias, outliers and repeated data is evaluated. The potential of the difference between root mean square error and mean absolute error for detecting outliers is explored, showing that this may be considered a necessary but not a sufficient condition of outlier presence. The usefulness of the approach for the evaluation of model performance is illustrated with case studies including those with similar goodness-of-fit indicators but distinct statistical interpretation, and others to analyze the effects of outliers, model bias and repeated data. This work does not intend to dictate rules on model goodness-of-fit assessment. It aims to provide modelers with improved, less subjective and practical model evaluation guidance and tools.},
	author = {Axel Ritter and Rafael Mu{\~n}oz-Carpena},
	doi = {https://doi.org/10.1016/j.jhydrol.2012.12.004},
	issn = {0022-1694},
	journal = {Journal of Hydrology},
	keywords = {Block bootstrapping, Coefficient of efficiency, Hypothesis testing, Model prediction error},
	pages = {33-45},
	title = {Performance evaluation of hydrological models: Statistical significance for reducing subjectivity in goodness-of-fit assessments},
	url = {https://www.sciencedirect.com/science/article/pii/S0022169412010608},
	volume = {480},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022169412010608},
	bdsk-url-2 = {https://doi.org/10.1016/j.jhydrol.2012.12.004}
}

@inproceedings{sgd-paper,
author = {Zhang, Tong},
title = {Solving large scale linear prediction problems using stochastic gradient descent algorithms},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015332},
doi = {10.1145/1015330.1015332},
abstract = {Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {116},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@webpage{sklearn-sgd,
	date-added = {2024-03-18 20:00:29 -0400},
	date-modified = {2024-03-18 20:02:41 -0400},
	title = {1.5. Stochastic Gradient Descent — scikit-learn 1.4.1 documentation.},
	lastchecked = {18 March 2024},
	month = {3},
	read = {0},
	url = {https://scikit-learn.org/stable/modules/sgd.html},
	year = {2024}
}

@inproceedings{adaptive-lr,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
title = {Pegasos: Primal Estimated sub-GrAdient SOlver for SVM},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273598},
doi = {10.1145/1273496.1273598},
abstract = {We describe and analyze a simple and effective iterative algorithm for solving the optimization problem cast by Support Vector Machines (SVM). Our method alternates between stochastic gradient descent steps and projection steps. We prove that the number of iterations required to obtain a solution of accuracy ε is \~{O}(1/ε). In contrast, previous analyses of stochastic gradient descent methods require Ω (1/ε2) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is \~{O} (d/(λε)), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach can seamlessly be adapted to employ non-linear kernels while working solely on the primal objective function. We demonstrate the efficiency and applicability of our approach by conducting experiments on large text classification problems, comparing our solver to existing state-of-the-art SVM solvers. For example, it takes less than 5 seconds for our solver to converge when solving a text classification problem from Reuters Corpus Volume 1 (RCV1) with 800,000 training examples.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{scikit-learn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@misc{mm-norm,
      title={Normalization: A Preprocessing Stage}, 
      author={S. Gopal Krishna Patro and Kishore Kumar Sahu},
      year={2015},
      eprint={1503.06462},
      archivePrefix={arXiv},
      primaryClass={cs.OH}
}

@InProceedings{momentum-paper,
  title = 	 {Towards understanding how momentum improves generalization in deep learning},
  author =       {Jelassi, Samy and Li, Yuanzhi},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {9965--10040},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {7},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/jelassi22a/jelassi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/jelassi22a.html},
  abstract = 	 {Stochastic gradient descent (SGD) with momentum is widely used for training modern deep learning architectures. While it is well-understood that using momentum can lead to faster convergence rate in various settings, it has also been observed that momentum yields higher generalization. Prior work argue that momentum stabilizes the SGD noise during training and this leads to higher generalization. In this paper, we adopt another perspective and first empirically show that gradient descent with momentum (GD+M) significantly improves generalization compared to gradient descent (GD) in some deep learning problems. From this observation, we formally study how momentum improves generalization. We devise a binary classification setting where a one-hidden layer (over-parameterized) convolutional neural network trained with GD+M provably generalizes better than the same network trained with GD, when both algorithms are similarly initialized. The key insight in our analysis is that momentum is beneficial in datasets where the examples share some feature but differ in their margin. Contrary to GD that memorizes the small margin data, GD+M still learns the feature in these data thanks to its historical gradients. Lastly, we empirically validate our theoretical findings.}
}

@article{mlp-paper,
	abstract = {A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.},
	author = {Geoffrey E. Hinton},
	doi = {https://doi.org/10.1016/0004-3702(89)90049-0},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {185-234},
	title = {Connectionist learning procedures},
	url = {https://www.sciencedirect.com/science/article/pii/0004370289900490},
	volume = {40},
	year = {1989},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370289900490},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(89)90049-0}
}

@InProceedings{deep-learning-paper,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {5},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
